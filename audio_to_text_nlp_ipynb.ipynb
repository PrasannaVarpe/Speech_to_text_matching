{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## liblaries for matching the name in the list\n",
        "!pip install thefuzz\n",
        "from thefuzz import fuzz\n",
        "from thefuzz import process\n",
        "\n",
        "## liblaries for speech to text conversion\n",
        "!pip install ffmpeg\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "import whisper\n",
        "\n",
        "## liblaries for natural language processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "0ZEO41grUdAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d2fd5a-f537-4d21-ed4b-25edd9d80392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting thefuzz\n",
            "  Downloading thefuzz-0.19.0-py2.py3-none-any.whl (17 kB)\n",
            "Installing collected packages: thefuzz\n",
            "Successfully installed thefuzz-0.19.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=3929d07a419ff0a3eb7a5ef566eb162408eddc8e79e5e8e44d7d7ec22c39e523\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-k3vr9cdx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-k3vr9cdx\n",
            "  Resolved https://github.com/openai/whisper.git to commit 248b6cb124225dd263bb9bd32d060b6517e067f8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230314)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798075 sha256=f2a0e51975f9b822aa626c8ad0949f23538f1c88cfd34a14f288358628c43de8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gbqdekz5/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230314 tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## First step is to convert audio files to text"
      ],
      "metadata": {
        "id": "cY6F7-mqW_Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_text():\n",
        "  files=['/content/1.wav','/content/2.wav','/content/3.wav','/content/4.wav','/content/5.wav']\n",
        "  model = whisper.load_model(\"base\")\n",
        "  rawtext=[]\n",
        "  for i in files:\n",
        "    result = model.transcribe(i)\n",
        "    rawtext.append(result[\"text\"])\n",
        "  return rawtext"
      ],
      "metadata": {
        "id": "CoQwGCmzW_SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=audio_to_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nX49uR4W_an",
        "outputId": "eeb0af4a-948f-4bb8-fba4-549e338c4cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Printing the raw_text just to see how our engine has performed\n",
        "\n",
        "print(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIRvMbrRNkkn",
        "outputId": "d78c9334-08a8-44ec-8ecc-e32603f8fd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\" you Hello, you've reached Jessica Parivar. Please leave your name.\", ' you This is Bart Brock. Sorry to answer your call, but if you leave your name', \" you Hi, this is Jacqueline sorry, I'm Mr. Call, so free to leave a message.\", \" you Hi, you reach Julian, I'll leave a message and I'll get back to you soon.\", \" you you've reached Matthew Q. Watch's voice panel please\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## second step is to process the raw data with natural language processing has two process A]Tokenization & B]Stopwords removal"
      ],
      "metadata": {
        "id": "6SP65CmRG-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## A] Tokenizing the raw_text\n",
        "\n",
        "def tokenizing_raw_text(raw_text):\n",
        "  tokenized_text=[]\n",
        "  for i in raw_text:\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokenized_text.append(tokenizer.tokenize(i)) \n",
        "  return tokenized_text\n",
        "\n",
        "## B] Removing Stopwords\n",
        "\n",
        "## downloading and storing stopwords in an itterable\n",
        "nltk.download('stopwords')\n",
        "stopwords=stopwords.words('english')\n",
        "\n",
        "def stopwords_removal():\n",
        "  tokenized_text=tokenizing_raw_text(raw_text)\n",
        "  data=[]\n",
        "  for i in tokenized_text:\n",
        "    text = [each_string.lower() for each_string in i]\n",
        "    newText = [word for word in text if not word in stopwords]\n",
        "    data.append(newText)\n",
        "  return data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IreF6UYxG-Tr",
        "outputId": "2a4ffd60-270f-4d7e-a224-ff8bfa215d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we have our final processed data to match with the name\n",
        "\n",
        "final_data=stopwords_removal()\n",
        "final_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkScCFOgG-W8",
        "outputId": "051da8d0-d23e-4708-89b0-dcd2cf81b361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'reached', 'jessica', 'parivar', 'please', 'leave', 'name'],\n",
              " ['bart', 'brock', 'sorry', 'answer', 'call', 'leave', 'name'],\n",
              " ['hi', 'jacqueline', 'sorry', 'mr', 'call', 'free', 'leave', 'message'],\n",
              " ['hi', 'reach', 'julian', 'leave', 'message', 'get', 'back', 'soon'],\n",
              " ['reached', 'matthew', 'q', 'watch', 'voice', 'panel', 'please']]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## last step is to find the name within final data and print the number"
      ],
      "metadata": {
        "id": "QhGEka1lG-cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## first we will convert the data to dictionary to print the number in end\n",
        "\n",
        "def list_to_dict(final_data):\n",
        "  liner=len(final_data)+1\n",
        "  line=range(1,liner)\n",
        "  res = dict(zip(line,final_data))\n",
        "  return res\n",
        "\n",
        "## Printing the number in the final step\n",
        "\n",
        "def finding_no():\n",
        "  res=list_to_dict(final_data)\n",
        "  full_name = 'jacqueline'\n",
        "  for k in res.values():\n",
        "    ratio=process.extractOne(full_name,k,scorer=fuzz.ratio)\n",
        "    no=ratio[-1]\n",
        "    if no >= 75:\n",
        "      a=k\n",
        "      value = {i for i in res if res[i]==a}\n",
        "      print(\"press:\",value)\n",
        "\n"
      ],
      "metadata": {
        "id": "U0m80g7OUdDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Finally printing the number\n",
        "\n",
        "finding_no()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFQ2kBdNnp-P",
        "outputId": "f8878890-3807-4a01-8796-38dc90557564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "press: {3}\n"
          ]
        }
      ]
    }
  ]
}